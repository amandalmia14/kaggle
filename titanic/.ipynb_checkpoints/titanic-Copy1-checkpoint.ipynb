{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('titanic/train.csv')\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "       391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "       404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "       417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "       430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "       443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "       456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "       469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
       "       482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494,\n",
       "       495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507,\n",
       "       508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520,\n",
       "       521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n",
       "       534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546,\n",
       "       547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
       "       560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572,\n",
       "       573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585,\n",
       "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598,\n",
       "       599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611,\n",
       "       612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624,\n",
       "       625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637,\n",
       "       638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650,\n",
       "       651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663,\n",
       "       664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676,\n",
       "       677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689,\n",
       "       690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702,\n",
       "       703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715,\n",
       "       716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728,\n",
       "       729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n",
       "       742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754,\n",
       "       755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767,\n",
       "       768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780,\n",
       "       781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793,\n",
       "       794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806,\n",
       "       807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819,\n",
       "       820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832,\n",
       "       833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845,\n",
       "       846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858,\n",
       "       859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871,\n",
       "       872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884,\n",
       "       885, 886, 887, 888, 889, 890, 891])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there is a NaN value preset or not.\n",
    "data_raw['PassengerId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['Survived'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['Pclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.  , 38.  , 26.  , 35.  ,   nan, 54.  ,  2.  , 27.  , 14.  ,\n",
       "        4.  , 58.  , 20.  , 39.  , 55.  , 31.  , 34.  , 15.  , 28.  ,\n",
       "        8.  , 19.  , 40.  , 66.  , 42.  , 21.  , 18.  ,  3.  ,  7.  ,\n",
       "       49.  , 29.  , 65.  , 28.5 ,  5.  , 11.  , 45.  , 17.  , 32.  ,\n",
       "       16.  , 25.  ,  0.83, 30.  , 33.  , 23.  , 24.  , 46.  , 59.  ,\n",
       "       71.  , 37.  , 47.  , 14.5 , 70.5 , 32.5 , 12.  ,  9.  , 36.5 ,\n",
       "       51.  , 55.5 , 40.5 , 44.  ,  1.  , 61.  , 56.  , 50.  , 36.  ,\n",
       "       45.5 , 20.5 , 62.  , 41.  , 52.  , 63.  , 23.5 ,  0.92, 43.  ,\n",
       "       60.  , 10.  , 64.  , 13.  , 48.  ,  0.75, 53.  , 57.  , 80.  ,\n",
       "       70.  , 24.5 ,  6.  ,  0.67, 30.5 ,  0.42, 34.5 , 74.  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN value present\n",
    "data_raw['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 4, 2, 5, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['SibSp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 5, 3, 4, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['Parch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.25  ,  71.2833,   7.925 ,  53.1   ,   8.05  ,   8.4583,\n",
       "        51.8625,  21.075 ,  11.1333,  30.0708,  16.7   ,  26.55  ,\n",
       "        31.275 ,   7.8542,  16.    ,  29.125 ,  13.    ,  18.    ,\n",
       "         7.225 ,  26.    ,   8.0292,  35.5   ,  31.3875, 263.    ,\n",
       "         7.8792,   7.8958,  27.7208, 146.5208,   7.75  ,  10.5   ,\n",
       "        82.1708,  52.    ,   7.2292,  11.2417,   9.475 ,  21.    ,\n",
       "        41.5792,  15.5   ,  21.6792,  17.8   ,  39.6875,   7.8   ,\n",
       "        76.7292,  61.9792,  27.75  ,  46.9   ,  80.    ,  83.475 ,\n",
       "        27.9   ,  15.2458,   8.1583,   8.6625,  73.5   ,  14.4542,\n",
       "        56.4958,   7.65  ,  29.    ,  12.475 ,   9.    ,   9.5   ,\n",
       "         7.7875,  47.1   ,  15.85  ,  34.375 ,  61.175 ,  20.575 ,\n",
       "        34.6542,  63.3583,  23.    ,  77.2875,   8.6542,   7.775 ,\n",
       "        24.15  ,   9.825 ,  14.4583, 247.5208,   7.1417,  22.3583,\n",
       "         6.975 ,   7.05  ,  14.5   ,  15.0458,  26.2833,   9.2167,\n",
       "        79.2   ,   6.75  ,  11.5   ,  36.75  ,   7.7958,  12.525 ,\n",
       "        66.6   ,   7.3125,  61.3792,   7.7333,  69.55  ,  16.1   ,\n",
       "        15.75  ,  20.525 ,  55.    ,  25.925 ,  33.5   ,  30.6958,\n",
       "        25.4667,  28.7125,   0.    ,  15.05  ,  39.    ,  22.025 ,\n",
       "        50.    ,   8.4042,   6.4958,  10.4625,  18.7875,  31.    ,\n",
       "       113.275 ,  27.    ,  76.2917,  90.    ,   9.35  ,  13.5   ,\n",
       "         7.55  ,  26.25  ,  12.275 ,   7.125 ,  52.5542,  20.2125,\n",
       "        86.5   , 512.3292,  79.65  , 153.4625, 135.6333,  19.5   ,\n",
       "        29.7   ,  77.9583,  20.25  ,  78.85  ,  91.0792,  12.875 ,\n",
       "         8.85  , 151.55  ,  30.5   ,  23.25  ,  12.35  , 110.8833,\n",
       "       108.9   ,  24.    ,  56.9292,  83.1583, 262.375 ,  14.    ,\n",
       "       164.8667, 134.5   ,   6.2375,  57.9792,  28.5   , 133.65  ,\n",
       "        15.9   ,   9.225 ,  35.    ,  75.25  ,  69.3   ,  55.4417,\n",
       "       211.5   ,   4.0125, 227.525 ,  15.7417,   7.7292,  12.    ,\n",
       "       120.    ,  12.65  ,  18.75  ,   6.8583,  32.5   ,   7.875 ,\n",
       "        14.4   ,  55.9   ,   8.1125,  81.8583,  19.2583,  19.9667,\n",
       "        89.1042,  38.5   ,   7.725 ,  13.7917,   9.8375,   7.0458,\n",
       "         7.5208,  12.2875,   9.5875,  49.5042,  78.2667,  15.1   ,\n",
       "         7.6292,  22.525 ,  26.2875,  59.4   ,   7.4958,  34.0208,\n",
       "        93.5   , 221.7792, 106.425 ,  49.5   ,  71.    ,  13.8625,\n",
       "         7.8292,  39.6   ,  17.4   ,  51.4792,  26.3875,  30.    ,\n",
       "        40.125 ,   8.7125,  15.    ,  33.    ,  42.4   ,  15.55  ,\n",
       "        65.    ,  32.3208,   7.0542,   8.4333,  25.5875,   9.8417,\n",
       "         8.1375,  10.1708, 211.3375,  57.    ,  13.4167,   7.7417,\n",
       "         9.4833,   7.7375,   8.3625,  23.45  ,  25.9292,   8.6833,\n",
       "         8.5167,   7.8875,  37.0042,   6.45  ,   6.95  ,   8.3   ,\n",
       "         6.4375,  39.4   ,  14.1083,  13.8583,  50.4958,   5.    ,\n",
       "         9.8458,  10.5167])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['Fare'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n",
       "       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n",
       "       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n",
       "       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n",
       "       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n",
       "       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n",
       "       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n",
       "       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n",
       "       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n",
       "       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n",
       "       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n",
       "       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n",
       "       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n",
       "       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n",
       "       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n",
       "       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n",
       "       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n",
       "       'C148'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN value present\n",
    "data_raw['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN value present\n",
    "data_raw['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27117366, 0.4722292 , 0.32143755, 0.43453129, 0.36792055,\n",
       "       0.67328474, 0.01985423, 0.33400352, 0.17064589, 0.04498618,\n",
       "       0.72354863, 0.24604172, 0.48479517, 0.68585072, 0.3842674 ,\n",
       "       0.42196532, 0.18321186, 0.34656949, 0.09525006, 0.23347575,\n",
       "       0.49736115, 0.8240764 , 0.52249309, 0.25860769, 0.22090978,\n",
       "       0.03242021, 0.08268409, 0.61045489, 0.35913546, 0.81151043,\n",
       "       0.35285248, 0.05755215, 0.13294798, 0.560191  , 0.2083438 ,\n",
       "       0.39683338, 0.19577783, 0.30887158, 0.00515205, 0.37170143,\n",
       "       0.40939935, 0.28373963, 0.2963056 , 0.57275697, 0.7361146 ,\n",
       "       0.88690626, 0.45966323, 0.58532295, 0.17692888, 0.88062327,\n",
       "       0.40311636, 0.14551395, 0.10781603, 0.45338025, 0.63558683,\n",
       "       0.6921337 , 0.50364413, 0.54762503, 0.00728826, 0.76124654,\n",
       "       0.69841669, 0.62302086, 0.44709726, 0.56647399, 0.2523247 ,\n",
       "       0.77381252, 0.50992712, 0.6481528 , 0.78637849, 0.29002262,\n",
       "       0.00628299, 0.53505906, 0.74868057, 0.12038201, 0.79894446,\n",
       "       0.15807992, 0.59788892, 0.00414677, 0.66071877, 0.71098266,\n",
       "       1.        , 0.87434029, 0.30258859, 0.07011812, 0.00314149,\n",
       "       0.37798442, 0.        , 0.4282483 , 0.92460417])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We got to know that there are few columsn which contains NAN values, so we need to replace them which proper \n",
    "# and staisfying values\n",
    "data_clean = data_raw\n",
    "#Age\n",
    "data_clean['Age'] = data_clean['Age'].fillna(data_clean['Age'].mean())\n",
    "normalized_df=(data_clean['Age']-data_clean['Age'].min())/(data_clean['Age'].max()-data_clean['Age'].min())\n",
    "data_clean['Age'] = normalized_df\n",
    "data_clean['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "normalized_df_f=(data_clean['Fare']-data_clean['Fare'].min())/(data_clean['Fare'].max()-data_clean['Fare'].min())\n",
    "data_clean['Fare'] = normalized_df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    260\n",
       "B    201\n",
       "E    147\n",
       "D    126\n",
       "A     77\n",
       "F     55\n",
       "G     24\n",
       "T      1\n",
       "Name: Deck, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cabin\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if big_string.find(substring) == 0:\n",
    "            return substring\n",
    "\n",
    "le = LabelEncoder()\n",
    "cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G']\n",
    "# Forward Fill to propagate the previous value forward\n",
    "data_clean['Cabin'] = data_clean['Cabin'].fillna(method='ffill')\n",
    "# Backward Fill to propagate the next value forward (if in case there is no forward value to be filled)\n",
    "data_clean['Cabin'] = data_clean['Cabin'].fillna(method='bfill')\n",
    "data_clean['Deck']= data_clean['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "data_clean['Deck'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    169\n",
       "Q     78\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embarked\n",
    "# Forward Fill to propagate the previous value forward\n",
    "data_clean['Embarked'] = data_clean['Embarked'].fillna(method='ffill')\n",
    "# Backward Fill to propagate the next value forward (if in case there is no forward value to be filled)\n",
    "data_clean['Embarked'] = data_clean['Embarked'].fillna(method='bfill')\n",
    "\n",
    "data_raw['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into categorial form\n",
    "data_clean['Sex_cat']= data_clean['Sex'].astype('category').cat.codes\n",
    "data_clean['Deck_cat']= data_clean['Deck'].astype('category').cat.codes\n",
    "data_clean['Embarked_cat']= data_clean['Embarked'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features\n",
    "data_clean['Family'] = data_clean['SibSp'] + data_clean['Parch'] + 1\n",
    "alone_or_family = []\n",
    "for data in data_clean['Family']:\n",
    "    if data == 1:\n",
    "        alone_or_family.append(0)\n",
    "    else:\n",
    "        alone_or_family.append(1)\n",
    "data_clean['Is_alone'] = alone_or_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_data\n",
    "# Not conisdering Fare and Ticket as it is irrelevant for the person survival factor\n",
    "X = data_clean[['Pclass','Sex_cat','Age','SibSp','Parch','Deck_cat','Embarked_cat', 'Is_alone', 'Family']]\n",
    "# X = data_clean[['Pclass','Sex_cat','Age','Deck_cat','Embarked_cat', 'Is_alone', 'Family']]\n",
    "Y = data_clean[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex_cat</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Deck_cat</th>\n",
       "      <th>Embarked_cat</th>\n",
       "      <th>Is_alone</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex_cat       Age  SibSp  Parch  Deck_cat  Embarked_cat  Is_alone  \\\n",
       "0       3        1  0.271174      1      0         2             2         1   \n",
       "1       1        0  0.472229      1      0         2             0         1   \n",
       "2       3        0  0.321438      0      0         2             2         0   \n",
       "3       1        0  0.434531      1      0         2             2         1   \n",
       "4       3        1  0.434531      0      0         2             2         0   \n",
       "\n",
       "   Family  \n",
       "0       2  \n",
       "1       2  \n",
       "2       1  \n",
       "3       2  \n",
       "4       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44307529313835026, 0.3466936034017244, 0.48088457087229597, 0.2858687393155517, 0.3915439122511488]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train, test) in enumerate(kfold.split(X, Y)):\n",
    " model.fit(X.iloc[train,:], Y.iloc[train,:])\n",
    " score = model.score(X.iloc[test,:], Y.iloc[test,:])\n",
    " scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working on test data set provided by the kaggle\n",
    "\n",
    "test_data_raw = pd.read_csv('titanic/test.csv')\n",
    "test_data_clean = test_data_raw\n",
    "\n",
    "# Pre-Processing as done for the training data\n",
    "\n",
    "#AGE\n",
    "test_data_clean['Age'] = test_data_clean['Age'].fillna(test_data_clean['Age'].mean())\n",
    "normalized_df=(test_data_clean['Age']-test_data_clean['Age'].min())/(test_data_clean['Age'].max()-test_data_clean['Age'].min())\n",
    "test_data_clean['Age'] = normalized_df\n",
    "\n",
    "# Fare\n",
    "normalized_df_fare=(data_clean['Fare']-data_clean['Fare'].min())/(data_clean['Fare'].max()-data_clean['Fare'].min())\n",
    "data_clean['Fare'] = normalized_df_fare\n",
    "\n",
    "#CABIN\n",
    "# Forward Fill to propagate the previous value forward\n",
    "test_data_clean['Cabin'] = test_data_clean['Cabin'].fillna(method='ffill')\n",
    "# Backward Fill to propagate the next value forward (if in case there is no forward value to be filled)\n",
    "test_data_clean['Cabin'] = test_data_clean['Cabin'].fillna(method='bfill')\n",
    "test_data_clean['Deck']= test_data_clean['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "test_data_clean['Deck'].value_counts()\n",
    "\n",
    "# EMBARKED\n",
    "# Forward Fill to propagate the previous value forward\n",
    "test_data_clean['Embarked'] = test_data_clean['Embarked'].fillna(method='ffill')\n",
    "# Backward Fill to propagate the next value forward (if in case there is no forward value to be filled)\n",
    "test_data_clean['Embarked'] = test_data_clean['Embarked'].fillna(method='bfill')\n",
    "\n",
    "# Adding new features\n",
    "test_data_clean['Family'] = test_data_clean['SibSp'] + test_data_clean['Parch'] + 1\n",
    "alone_or_family_test = []\n",
    "for data in test_data_clean['Family']:\n",
    "    if data == 1:\n",
    "        alone_or_family_test.append(0)\n",
    "    else:\n",
    "        alone_or_family_test.append(1)\n",
    "test_data_clean['Is_alone'] = alone_or_family_test\n",
    "\n",
    "# Converting into categorial form\n",
    "test_data_clean['Sex_cat']= test_data_clean['Sex'].astype('category').cat.codes\n",
    "test_data_clean['Deck_cat']= test_data_clean['Deck'].astype('category').cat.codes\n",
    "test_data_clean['Embarked_cat']= test_data_clean['Embarked'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "X_test = test_data_clean[['Pclass','Sex_cat','Age','SibSp','Parch','Deck_cat','Embarked_cat','Is_alone','Family']]\n",
    "# X_test = test_data_clean[['Pclass','Sex_cat','Age','Deck_cat','Embarked_cat','Is_alone','Family']]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(X_test)\n",
    "predicted_output = []\n",
    "for i in output:\n",
    "    if list(i)[0] > 0.5:\n",
    "        predicted_output.append(1)\n",
    "    else:\n",
    "        predicted_output.append(0)\n",
    "passenger_id_test = test_data_raw['PassengerId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(passenger_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gender_submission.csv', 'w') as outcsv:\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow([\"PassengerId\", \"Survived\"])\n",
    "    writer.writerows(zip(passenger_id_test, predicted_output))                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
